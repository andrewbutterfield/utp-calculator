\section{Introduction}\label{sec:Intro}

\subsection{Motivation}

The development of a Unifying Theory of Programming (UTP)
can involve a number of false starts,
as alphabet variables are chosen
and semantics and healthiness conditions are defined.
Typically, some calculations done just to check that everything
works reveal problems with the theory.
So an iteration occurs by revising the basic definitions,
and attempting the calculations again.

We have recently started to explore using UTP
to describe shared-variable concurrency,
by adapting the work of the  UTP semantics for Unifying Theories
of Parallel Programming (UTPP) \cite{DBLP:conf/icfem/WoodcockH02}.
We have reworked it to use a system for generating unique labels,
in order to give a slight improvement to the compositionality
of the semantics. This we call a Unifying Theory of Concurrent Programming
(UTCP) and some details of this were recently published\cite{conf/tase/BMN16}.

We give a very brief overview here of this theory.
The language assumes atomic actions ($A$) that modify a generic state,
and four composition operators: sequential ($\lseq$), parallel ($\parallel$),
non-deterministic choice ($\pcond$) and non-deterministic iterations ($\piter{}$)
\[
   P ::= A \mid P \pseq P \mid P \parallel P \mid P \pcond P \mid \piter P
\]
Note that this is essentially the same as the baseline ``Command'' language
in the Views paper\cite{conf/popl/Dinsdale-YoungBGPY13}.
So our UTCP theory is based on a non-homogeneous relation
with alphabet $s$, $s'$, $ls$, $ls'$, $g$, $in$, and $out$.
Observations $s$ and $ls$ (and dashed counterparts)
represent the shared state and the global label-set used for control flow.
Observations $g$, $in$ and $out$ are static parameters that do not change
at run time.


The theory with its somewhat unusual arrangement of observation variables
did \emph{not} emerge as an immediate and obvious solution,
but as a result of many trial calculations.
These trial calculations exposed two problems:
The first problem was relatively minor
---a lot of the semantics talks about the presence of various
labels in the global label-set, or alternatively, their  absence,
and of modifications to that set.
This resulted in very wide expressions,
so some shorthand notations with pretty-printing support were introduced.

The second problem was more serious.
It often took long painstaking calculations to discover problems
with a proposed version of the theory,
typically when trying to calculate outcomes associated with
$A \pseq B$ and then  $A \parallel B$
(400 lines of \LaTeX\,7 full pages of output)
With choice and iteration to be covered,
it was becoming very clear that a better way was required
for checking semantic outcomes.

To be specific, the calculator described in this paper
is intended to be used for \emph{calculation}, and not \emph{theorem proving}.
In particular, it is designed to help solve the following problem:
Given a predicate, typically containing some simple
atomic actions connected using the composite constructs,
try to reduce this down
using equational reasoning,
to a predicate over the observation
variables of the theory without any explicit occurrence of the language constructs.
This can then be inspected to see if the assertions made about the observables
are correct.
For example, in the UTCP theory as presented in \cite{conf/tase/BMN16},
our starting predicate might be
\[
  run(\A(A) \pseq \A(B))
\]
where $A$ and $B$ are atomic actions that only mention $s$ and $s'$,
while $\A(\_)$ lifts those to full program behaviour over the full alphabet.
The predicate transformer $run$ effectively drives the execution
of the program predicate given as its argument.
What we would hope to result from the test calculation would be
something of the form
\[
  (A \seq B) \land ls' = \setof{\ell_{g:}}
\]
We have the standard UTP sequential composition of $A$ and $B$ defined on $s,s'$,
and an assertion that the termination label $\ell_{g:}$ is in the final label-set.
Both the starting predicate and the final result have free variables
and are not theorems
---they need to be both falsifiable and satisfiable.
What matters is precisely when it is one or the other.
That means the use of tautology checkers or counter-example discovery tools
is not helpful.


If we consider the reasoning processes used in the development
and deployment of a theory, we can see a spectrum ranging from informal,
through to fully mechanised: hand calculation; simulation; proof assistant;
and automated theorem provers.
The level of detail, complexity, and rigour rises as we proceed along the spectrum.
The calculator described here is designed
to assist with the exploratory hand-calculation phase early on,
by making it easier to calculate, and to manually check the outcomes.
It is not intended to provide the soundness guarantees that are quite rightly
expected from the tools further along the spectrum.


\subsection{Structure of this paper}

In Sect. \ref{sec:Related} we discuss related work
to justify our decision to develop the calculator.
The key design decisions and tool architecture is then described
in Sect. \ref{sec:Design}.
Three key components of the system are the discussed:
Dictionaries (Sect. \ref{sec:Dictionaries});
Expressions (Sect. \ref{sec:Expressions}); and
Predicates (Sect. \ref{sec:Predicates}).
In Sect. \ref{sec:Laws} we describe how to encode laws
and then conclude (Sect. \ref{sec:Conc}).
